{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eeg.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "dUIZ65Tu0k-7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mLcUM3Tn0lUQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Install a kaggle-cli to download the data."
      ]
    },
    {
      "metadata": {
        "id": "AqQXs6I321Cv",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 1014
        },
        "outputId": "0ec9cd32-a587-44c0-f5a8-9d05001d6209",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525529067081,
          "user_tz": -120,
          "elapsed": 11310,
          "user": {
            "displayName": "Norman Di Palo",
            "photoUrl": "//lh5.googleusercontent.com/-hwpkiYzSctg/AAAAAAAAAAI/AAAAAAAAAkg/I59DXttMdKw/s50-c-k-no/photo.jpg",
            "userId": "109405231849327541723"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install kaggle-cli"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kaggle-cli\n",
            "  Downloading https://files.pythonhosted.org/packages/67/61/710d02460bc4367ffd1f5e71cd9c031fb278f78aa0e8e32ca9dd99a2add8/kaggle-cli-0.12.13.tar.gz\n",
            "Collecting cliff<2.9,>=2.8.0 (from kaggle-cli)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/e8/140ad9b5826920a8b85c187095e7725b87b913fc40243aa66dd04e9d82d6/cliff-2.8.1.tar.gz (73kB)\n",
            "\u001b[K    100% |████████████████████████████████| 81kB 2.8MB/s \n",
            "\u001b[?25hCollecting MechanicalSoup<0.9,>=0.7.0 (from kaggle-cli)\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/2e/f63ed26b51e36efa4cc22cad18187fcb0a253f756d548c96bb931f13de98/MechanicalSoup-0.8.0-py2.py3-none-any.whl\n",
            "Collecting lxml<4.1,>=4.0.0 (from kaggle-cli)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/b5/4c6995f8f259f0858f79460e6d277888f8498ce1c1a466dfbb24f06ba83f/lxml-4.0.0-cp36-cp36m-manylinux1_x86_64.whl (5.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.3MB 6.4MB/s \n",
            "\u001b[?25hCollecting cssselect<1.1,>=1.0.1 (from kaggle-cli)\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/44/25b7283e50585f0b4156960691d951b05d061abf4a714078393e51929b30/cssselect-1.0.3-py2.py3-none-any.whl\n",
            "Collecting configparser (from kaggle-cli)\n",
            "  Downloading https://files.pythonhosted.org/packages/7c/69/c2ce7e91c89dc073eb1aa74c0621c3eefbffe8216b3f9af9d3885265c01c/configparser-3.5.0.tar.gz\n",
            "Collecting progressbar2<3.35,>=3.34.3 (from kaggle-cli)\n",
            "  Downloading https://files.pythonhosted.org/packages/87/31/b984e17bcc7491c1baeda3906fe3abc14cb5cd5dbd046ab46d9fc7a2edfd/progressbar2-3.34.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: beautifulsoup4<4.7,>=4.6.0 in /usr/local/lib/python3.6/dist-packages (from kaggle-cli) (4.6.0)\n",
            "Collecting pbr!=2.1.0,>=2.0.0 (from cliff<2.9,>=2.8.0->kaggle-cli)\n",
            "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/ba/f95e3ec83f93919b1437028e989cf3fa5ff4f5cae4a1f62255f71deddb5b/pbr-4.0.2-py2.py3-none-any.whl (98kB)\n",
            "\u001b[K    100% |████████████████████████████████| 102kB 22.0MB/s \n",
            "\u001b[?25hCollecting cmd2>=0.6.7 (from cliff<2.9,>=2.8.0->kaggle-cli)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/02/faa35704449a68f18db39ee9a047e32c8b569e672daadfef3501259fe6bb/cmd2-0.8.5-py2.py3-none-any.whl (51kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 21.5MB/s \n",
            "\u001b[?25hCollecting PrettyTable<0.8,>=0.7.1 (from cliff<2.9,>=2.8.0->kaggle-cli)\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/30/4b0746848746ed5941f052479e7c23d2b56d174b82f4fd34a25e389831f5/prettytable-0.7.2.tar.bz2\n",
            "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from cliff<2.9,>=2.8.0->kaggle-cli) (2.2.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cliff<2.9,>=2.8.0->kaggle-cli) (1.11.0)\n",
            "Collecting stevedore>=1.20.0 (from cliff<2.9,>=2.8.0->kaggle-cli)\n",
            "  Downloading https://files.pythonhosted.org/packages/17/6b/3b7d6d08b2ab3e5ef09e01c9f7b3b590ee135f289bb94553419e40922c25/stevedore-1.28.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML>=3.10.0 in /usr/local/lib/python3.6/dist-packages (from cliff<2.9,>=2.8.0->kaggle-cli) (3.12)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from MechanicalSoup<0.9,>=0.7.0->kaggle-cli) (2.18.4)\n",
            "Collecting python-utils>=2.1.0 (from progressbar2<3.35,>=3.34.3->kaggle-cli)\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/a0/19119d8b7c05be49baf6c593f11c432d571b70d805f2fe94c0585e55e4c8/python_utils-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: wcwidth; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from cmd2>=0.6.7->cliff<2.9,>=2.8.0->kaggle-cli) (0.1.7)\n",
            "Collecting pyperclip (from cmd2>=0.6.7->cliff<2.9,>=2.8.0->kaggle-cli)\n",
            "  Downloading https://files.pythonhosted.org/packages/5b/06/86e3c6a55cacef0e4ec7c25379ff7fcd1a88fd939ecefd442b535c792fa4/pyperclip-1.6.0.tar.gz\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->MechanicalSoup<0.9,>=0.7.0->kaggle-cli) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->MechanicalSoup<0.9,>=0.7.0->kaggle-cli) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->MechanicalSoup<0.9,>=0.7.0->kaggle-cli) (2018.4.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->MechanicalSoup<0.9,>=0.7.0->kaggle-cli) (3.0.4)\n",
            "Building wheels for collected packages: kaggle-cli, cliff, configparser, PrettyTable, pyperclip\n",
            "  Running setup.py bdist_wheel for kaggle-cli ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/d5/bb/10/c1dd1b08c7433c943cb55c46367ae3f891415e8a37300ff8a7\n",
            "  Running setup.py bdist_wheel for cliff ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/50/00/6d/d4aeb5ccdd47dd76800592b26f943e4959bc705b2d4e6e54e1\n",
            "  Running setup.py bdist_wheel for configparser ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/a3/61/79/424ef897a2f3b14684a7de5d89e8600b460b89663e6ce9d17c\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  Running setup.py bdist_wheel for PrettyTable ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/80/34/1c/3967380d9676d162cb59513bd9dc862d0584e045a162095606\n",
            "  Running setup.py bdist_wheel for pyperclip ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/a7/68/ef/ab1ef01625334f10a979e9fe5ef79e1c4fd4cdf4b710a0067b\n",
            "Successfully built kaggle-cli cliff configparser PrettyTable pyperclip\n",
            "Installing collected packages: pbr, pyperclip, cmd2, PrettyTable, stevedore, cliff, MechanicalSoup, lxml, cssselect, configparser, python-utils, progressbar2, kaggle-cli\n",
            "Successfully installed MechanicalSoup-0.8.0 PrettyTable-0.7.2 cliff-2.8.1 cmd2-0.8.5 configparser-3.5.0 cssselect-1.0.3 kaggle-cli-0.12.13 lxml-4.0.0 pbr-4.0.2 progressbar2-3.34.3 pyperclip-1.6.0 python-utils-2.3.0 stevedore-1.28.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Tyytf5OH0rtp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Download the data (insert your username and password)"
      ]
    },
    {
      "metadata": {
        "id": "61LXLiY82-fA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "1f5b2157-1680-42ea-f529-6f512a4c1e75",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525529112335,
          "user_tz": -120,
          "elapsed": 32880,
          "user": {
            "displayName": "Norman Di Palo",
            "photoUrl": "//lh5.googleusercontent.com/-hwpkiYzSctg/AAAAAAAAAAI/AAAAAAAAAkg/I59DXttMdKw/s50-c-k-no/photo.jpg",
            "userId": "109405231849327541723"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!kg download -u INSERT-USERNAME -p INSERT-PASSWORD -c grasp-and-lift-eeg-detection -f train.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading https://www.kaggle.com/c/grasp-and-lift-eeg-detection/download/train.zip\r\n",
            "\n",
            "train.zip 100% |####################################| Time: 0:00:27  33.0 MiB/s\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5cX3CXhZ0wqS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Check the data and unzip it."
      ]
    },
    {
      "metadata": {
        "id": "rms8g6gQ5JxW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a46ba305-ea27-4ec4-efee-cd720a01ac4b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525529115126,
          "user_tz": -120,
          "elapsed": 1684,
          "user": {
            "displayName": "Norman Di Palo",
            "photoUrl": "//lh5.googleusercontent.com/-hwpkiYzSctg/AAAAAAAAAAI/AAAAAAAAAkg/I59DXttMdKw/s50-c-k-no/photo.jpg",
            "userId": "109405231849327541723"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datalab  train.zip\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rlnYcxb-4Oul",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 3465
        },
        "outputId": "3660a050-e067-496c-d8f8-00c0dfe780e8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525529154390,
          "user_tz": -120,
          "elapsed": 34033,
          "user": {
            "displayName": "Norman Di Palo",
            "photoUrl": "//lh5.googleusercontent.com/-hwpkiYzSctg/AAAAAAAAAAI/AAAAAAAAAkg/I59DXttMdKw/s50-c-k-no/photo.jpg",
            "userId": "109405231849327541723"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip train.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  train.zip\r\n",
            "   creating: train/\n",
            "  inflating: train/subj10_series1_data.csv  \n",
            "  inflating: train/subj10_series1_events.csv  \n",
            "  inflating: train/subj10_series2_data.csv  \n",
            "  inflating: train/subj10_series2_events.csv  \n",
            "  inflating: train/subj10_series3_data.csv  \n",
            "  inflating: train/subj10_series3_events.csv  \n",
            "  inflating: train/subj10_series4_data.csv  \n",
            "  inflating: train/subj10_series4_events.csv  \n",
            "  inflating: train/subj10_series5_data.csv  \n",
            "  inflating: train/subj10_series5_events.csv  \n",
            "  inflating: train/subj10_series6_data.csv  \n",
            "  inflating: train/subj10_series6_events.csv  \n",
            "  inflating: train/subj10_series7_data.csv  \n",
            "  inflating: train/subj10_series7_events.csv  \n",
            "  inflating: train/subj10_series8_data.csv  \n",
            "  inflating: train/subj10_series8_events.csv  \n",
            "  inflating: train/subj11_series1_data.csv  \n",
            "  inflating: train/subj11_series1_events.csv  \n",
            "  inflating: train/subj11_series2_data.csv  \n",
            "  inflating: train/subj11_series2_events.csv  \n",
            "  inflating: train/subj11_series3_data.csv  \n",
            "  inflating: train/subj11_series3_events.csv  \n",
            "  inflating: train/subj11_series4_data.csv  \n",
            "  inflating: train/subj11_series4_events.csv  \n",
            "  inflating: train/subj11_series5_data.csv  \n",
            "  inflating: train/subj11_series5_events.csv  \n",
            "  inflating: train/subj11_series6_data.csv  \n",
            "  inflating: train/subj11_series6_events.csv  \n",
            "  inflating: train/subj11_series7_data.csv  \n",
            "  inflating: train/subj11_series7_events.csv  \n",
            "  inflating: train/subj11_series8_data.csv  \n",
            "  inflating: train/subj11_series8_events.csv  \n",
            "  inflating: train/subj12_series1_data.csv  \n",
            "  inflating: train/subj12_series1_events.csv  \n",
            "  inflating: train/subj12_series2_data.csv  \n",
            "  inflating: train/subj12_series2_events.csv  \n",
            "  inflating: train/subj12_series3_data.csv  \n",
            "  inflating: train/subj12_series3_events.csv  \n",
            "  inflating: train/subj12_series4_data.csv  \n",
            "  inflating: train/subj12_series4_events.csv  \n",
            "  inflating: train/subj12_series5_data.csv  \n",
            "  inflating: train/subj12_series5_events.csv  \n",
            "  inflating: train/subj12_series6_data.csv  \n",
            "  inflating: train/subj12_series6_events.csv  \n",
            "  inflating: train/subj12_series7_data.csv  \n",
            "  inflating: train/subj12_series7_events.csv  \n",
            "  inflating: train/subj12_series8_data.csv  \n",
            "  inflating: train/subj12_series8_events.csv  \n",
            "  inflating: train/subj1_series1_data.csv  \n",
            "  inflating: train/subj1_series1_events.csv  \n",
            "  inflating: train/subj1_series2_data.csv  \n",
            "  inflating: train/subj1_series2_events.csv  \n",
            "  inflating: train/subj1_series3_data.csv  \n",
            "  inflating: train/subj1_series3_events.csv  \n",
            "  inflating: train/subj1_series4_data.csv  \n",
            "  inflating: train/subj1_series4_events.csv  \n",
            "  inflating: train/subj1_series5_data.csv  \n",
            "  inflating: train/subj1_series5_events.csv  \n",
            "  inflating: train/subj1_series6_data.csv  \n",
            "  inflating: train/subj1_series6_events.csv  \n",
            "  inflating: train/subj1_series7_data.csv  \n",
            "  inflating: train/subj1_series7_events.csv  \n",
            "  inflating: train/subj1_series8_data.csv  \n",
            "  inflating: train/subj1_series8_events.csv  \n",
            "  inflating: train/subj2_series1_data.csv  \n",
            "  inflating: train/subj2_series1_events.csv  \n",
            "  inflating: train/subj2_series2_data.csv  \n",
            "  inflating: train/subj2_series2_events.csv  \n",
            "  inflating: train/subj2_series3_data.csv  \n",
            "  inflating: train/subj2_series3_events.csv  \n",
            "  inflating: train/subj2_series4_data.csv  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r\n",
            "  inflating: train/subj2_series4_events.csv  \n",
            "  inflating: train/subj2_series5_data.csv  \n",
            "  inflating: train/subj2_series5_events.csv  \n",
            "  inflating: train/subj2_series6_data.csv  \n",
            "  inflating: train/subj2_series6_events.csv  \n",
            "  inflating: train/subj2_series7_data.csv  \n",
            "  inflating: train/subj2_series7_events.csv  \n",
            "  inflating: train/subj2_series8_data.csv  \n",
            "  inflating: train/subj2_series8_events.csv  \n",
            "  inflating: train/subj3_series1_data.csv  \n",
            "  inflating: train/subj3_series1_events.csv  \n",
            "  inflating: train/subj3_series2_data.csv  \n",
            "  inflating: train/subj3_series2_events.csv  \n",
            "  inflating: train/subj3_series3_data.csv  \n",
            "  inflating: train/subj3_series3_events.csv  \n",
            "  inflating: train/subj3_series4_data.csv  \n",
            "  inflating: train/subj3_series4_events.csv  \n",
            "  inflating: train/subj3_series5_data.csv  \n",
            "  inflating: train/subj3_series5_events.csv  \n",
            "  inflating: train/subj3_series6_data.csv  \n",
            "  inflating: train/subj3_series6_events.csv  \n",
            "  inflating: train/subj3_series7_data.csv  \n",
            "  inflating: train/subj3_series7_events.csv  \n",
            "  inflating: train/subj3_series8_data.csv  \n",
            "  inflating: train/subj3_series8_events.csv  \n",
            "  inflating: train/subj4_series1_data.csv  \n",
            "  inflating: train/subj4_series1_events.csv  \n",
            "  inflating: train/subj4_series2_data.csv  \n",
            "  inflating: train/subj4_series2_events.csv  \n",
            "  inflating: train/subj4_series3_data.csv  \n",
            "  inflating: train/subj4_series3_events.csv  \n",
            "  inflating: train/subj4_series4_data.csv  \n",
            "  inflating: train/subj4_series4_events.csv  \n",
            "  inflating: train/subj4_series5_data.csv  \n",
            "  inflating: train/subj4_series5_events.csv  \n",
            "  inflating: train/subj4_series6_data.csv  \n",
            "  inflating: train/subj4_series6_events.csv  \n",
            "  inflating: train/subj4_series7_data.csv  \n",
            "  inflating: train/subj4_series7_events.csv  \n",
            "  inflating: train/subj4_series8_data.csv  \n",
            "  inflating: train/subj4_series8_events.csv  \n",
            "  inflating: train/subj5_series1_data.csv  \n",
            "  inflating: train/subj5_series1_events.csv  \n",
            "  inflating: train/subj5_series2_data.csv  \n",
            "  inflating: train/subj5_series2_events.csv  \n",
            "  inflating: train/subj5_series3_data.csv  \n",
            "  inflating: train/subj5_series3_events.csv  \n",
            "  inflating: train/subj5_series4_data.csv  \n",
            "  inflating: train/subj5_series4_events.csv  \n",
            "  inflating: train/subj5_series5_data.csv  \n",
            "  inflating: train/subj5_series5_events.csv  \n",
            "  inflating: train/subj5_series6_data.csv  \n",
            "  inflating: train/subj5_series6_events.csv  \n",
            "  inflating: train/subj5_series7_data.csv  \n",
            "  inflating: train/subj5_series7_events.csv  \n",
            "  inflating: train/subj5_series8_data.csv  \n",
            "  inflating: train/subj5_series8_events.csv  \n",
            "  inflating: train/subj6_series1_data.csv  \n",
            "  inflating: train/subj6_series1_events.csv  \n",
            "  inflating: train/subj6_series2_data.csv  \n",
            "  inflating: train/subj6_series2_events.csv  \n",
            "  inflating: train/subj6_series3_data.csv  \n",
            "  inflating: train/subj6_series3_events.csv  \n",
            "  inflating: train/subj6_series4_data.csv  \n",
            "  inflating: train/subj6_series4_events.csv  \n",
            "  inflating: train/subj6_series5_data.csv  \n",
            "  inflating: train/subj6_series5_events.csv  \n",
            "  inflating: train/subj6_series6_data.csv  \n",
            "  inflating: train/subj6_series6_events.csv  \n",
            "  inflating: train/subj6_series7_data.csv  \n",
            "  inflating: train/subj6_series7_events.csv  \n",
            "  inflating: train/subj6_series8_data.csv  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  inflating: train/subj6_series8_events.csv  \n",
            "  inflating: train/subj7_series1_data.csv  \n",
            "  inflating: train/subj7_series1_events.csv  \n",
            "  inflating: train/subj7_series2_data.csv  \n",
            "  inflating: train/subj7_series2_events.csv  \n",
            "  inflating: train/subj7_series3_data.csv  \n",
            "  inflating: train/subj7_series3_events.csv  \n",
            "  inflating: train/subj7_series4_data.csv  \n",
            "  inflating: train/subj7_series4_events.csv  \n",
            "  inflating: train/subj7_series5_data.csv  \n",
            "  inflating: train/subj7_series5_events.csv  \n",
            "  inflating: train/subj7_series6_data.csv  \n",
            "  inflating: train/subj7_series6_events.csv  \n",
            "  inflating: train/subj7_series7_data.csv  \n",
            "  inflating: train/subj7_series7_events.csv  \n",
            "  inflating: train/subj7_series8_data.csv  \n",
            "  inflating: train/subj7_series8_events.csv  \n",
            "  inflating: train/subj8_series1_data.csv  \n",
            "  inflating: train/subj8_series1_events.csv  \n",
            "  inflating: train/subj8_series2_data.csv  \n",
            "  inflating: train/subj8_series2_events.csv  \n",
            "  inflating: train/subj8_series3_data.csv  \n",
            "  inflating: train/subj8_series3_events.csv  \n",
            "  inflating: train/subj8_series4_data.csv  \n",
            "  inflating: train/subj8_series4_events.csv  \n",
            "  inflating: train/subj8_series5_data.csv  \n",
            "  inflating: train/subj8_series5_events.csv  \n",
            "  inflating: train/subj8_series6_data.csv  \n",
            "  inflating: train/subj8_series6_events.csv  \n",
            "  inflating: train/subj8_series7_data.csv  \n",
            "  inflating: train/subj8_series7_events.csv  \n",
            "  inflating: train/subj8_series8_data.csv  \n",
            "  inflating: train/subj8_series8_events.csv  \n",
            "  inflating: train/subj9_series1_data.csv  \n",
            "  inflating: train/subj9_series1_events.csv  \n",
            "  inflating: train/subj9_series2_data.csv  \n",
            "  inflating: train/subj9_series2_events.csv  \n",
            "  inflating: train/subj9_series3_data.csv  \n",
            "  inflating: train/subj9_series3_events.csv  \n",
            "  inflating: train/subj9_series4_data.csv  \n",
            "  inflating: train/subj9_series4_events.csv  \n",
            "  inflating: train/subj9_series5_data.csv  \n",
            "  inflating: train/subj9_series5_events.csv  \n",
            "  inflating: train/subj9_series6_data.csv  \n",
            "  inflating: train/subj9_series6_events.csv  \n",
            "  inflating: train/subj9_series7_data.csv  \n",
            "  inflating: train/subj9_series7_events.csv  \n",
            "  inflating: train/subj9_series8_data.csv  \n",
            "  inflating: train/subj9_series8_events.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IIO8dioj01t1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Learning phase.\n",
        "Load several series of different subjects (a sample of the all dataset). With time_steps and subsample you decide how many temporal steps to take (e.g., 1000 sequential data points) and how to subsample them (e.g. take 1 every 100). We then define a convolutional neural network that computes features doing \"temporal convolutions\", i.e. it uses filters over both the different EEG measurments at a particular moment and future/past measurments. \\\\\n",
        "It uses a generator to load the data, that otherwise could be to heavy to store in memory (since it is multiplied by the time_steps value). Notice that the data points are all taken before the y value to predict to ensure a causal relationship. \\\\\n"
      ]
    },
    {
      "metadata": {
        "id": "AFoTOqC-4Rjl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 2091
        },
        "outputId": "d5f1e05a-787a-4c09-b1b4-a4e3056bd501",
        "executionInfo": {
          "status": "error",
          "timestamp": 1525539903799,
          "user_tz": -120,
          "elapsed": 30615,
          "user": {
            "displayName": "Norman Di Palo",
            "photoUrl": "//lh5.googleusercontent.com/-hwpkiYzSctg/AAAAAAAAAAI/AAAAAAAAAkg/I59DXttMdKw/s50-c-k-no/photo.jpg",
            "userId": "109405231849327541723"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "load = 0 #use 0 if the data is already loaded\n",
        "\n",
        "time_steps = 1000\n",
        "\n",
        "subsample = 100\n",
        "\n",
        "x_paths = [\"train/subj1_series1_data.csv\", \"train/subj1_series2_data.csv\", \"train/subj1_series3_data.csv\", \"train/subj1_series4_data.csv\", \\\n",
        "          \"train/subj2_series1_data.csv\", \"train/subj2_series2_data.csv\", \"train/subj2_series3_data.csv\", \"train/subj2_series4_data.csv\", \\\n",
        "          \"train/subj3_series1_data.csv\", \"train/subj3_series2_data.csv\", \"train/subj3_series3_data.csv\", \"train/subj3_series4_data.csv\", \\\n",
        "          \"train/subj4_series1_data.csv\", \"train/subj4_series2_data.csv\", \"train/subj4_series3_data.csv\", \"train/subj4_series4_data.csv\",\\\n",
        "          \"train/subj5_series1_data.csv\", \"train/subj5_series2_data.csv\", \"train/subj5_series3_data.csv\", \"train/subj5_series4_data.csv\",\\\n",
        "          \"train/subj6_series1_data.csv\", \"train/subj6_series2_data.csv\", \"train/subj6_series3_data.csv\", \"train/subj6_series4_data.csv\"]\n",
        "y_paths = [\"train/subj1_series1_events.csv\", \"train/subj1_series2_events.csv\", \"train/subj1_series3_events.csv\", \"train/subj1_series4_events.csv\", \\\n",
        "          \"train/subj2_series1_events.csv\", \"train/subj2_series2_events.csv\", \"train/subj2_series3_events.csv\", \"train/subj2_series4_events.csv\", \\\n",
        "          \"train/subj3_series1_events.csv\", \"train/subj3_series2_events.csv\", \"train/subj3_series3_events.csv\", \"train/subj3_series4_events.csv\", \\\n",
        "          \"train/subj4_series1_events.csv\", \"train/subj4_series2_events.csv\", \"train/subj4_series3_events.csv\", \"train/subj4_series4_events.csv\", \\\n",
        "          \"train/subj5_series1_events.csv\", \"train/subj5_series2_events.csv\", \"train/subj5_series3_events.csv\", \"train/subj5_series4_events.csv\", \\\n",
        "          \"train/subj6_series1_events.csv\", \"train/subj6_series2_events.csv\", \"train/subj6_series3_events.csv\", \"train/subj6_series4_events.csv\"]\n",
        "\n",
        "\n",
        "if load:\n",
        "  x_data = []\n",
        "  for x_path in x_paths:\n",
        "    with open(x_path) as file:\n",
        "        for line in file:\n",
        "            if line[0] == \"i\": continue\n",
        "            line_array = []\n",
        "            for word in range(len(line.split(','))-1):\n",
        "                word+=1\n",
        "                line_array.append(int(line.split(',')[word]))\n",
        "            line_array = np.asarray(line_array)\n",
        "            x_data.append(line_array)\n",
        "  x_data = np.asarray(x_data)    \n",
        "        \n",
        "  \n",
        "  y_data = []\n",
        "  for y_path in y_paths:\n",
        "    with open(y_path) as file:\n",
        "        for line in file:\n",
        "            if line[0] == \"i\": continue\n",
        "            line_array = []\n",
        "            for word in range(len(line.split(','))-1):\n",
        "                word+=1\n",
        "                line_array.append(int(line.split(',')[word]))\n",
        "            line_array = np.asarray(line_array)\n",
        "            y_data.append(line_array)\n",
        "  y_data = np.asarray(y_data)    \n",
        "        \n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM, CuDNNLSTM, BatchNormalization, Conv2D, Flatten, MaxPooling2D, Dropout\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "model = Sequential()\n",
        "#model.add(CuDNNLSTM(128, input_shape = (time_steps//subsample, 32)))\n",
        "model.add(Conv2D(filters = 64, kernel_size = (7,7), padding = \"same\", activation = \"relu\", input_shape = (time_steps//subsample, 32, 1)))\n",
        "model.add(BatchNormalization())\n",
        "#model.add(MaxPooling2D(pool_size = (3,3)))\n",
        "model.add(Conv2D(filters = 64, kernel_size = (5,5), padding = \"same\", activation = \"relu\", input_shape = (time_steps//subsample, 32, 1)))\n",
        "model.add(BatchNormalization())\n",
        "#model.add(MaxPooling2D(pool_size = (3,3)))\n",
        "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = \"same\", activation = \"relu\", input_shape = (time_steps//subsample, 32, 1)))\n",
        "model.add(BatchNormalization())\n",
        "#model.add(MaxPooling2D(pool_size = (3,3)))\n",
        "model.add(Flatten())\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(32, activation = \"relu\"))\n",
        "model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.2))\n",
        "model.add(Dense(6, activation = \"sigmoid\"))\n",
        "\n",
        "\n",
        "adam = Adam(lr = 0.001)\n",
        "\n",
        "model.compile(optimizer = adam, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "def generator(batch_size):\n",
        "    while 1:\n",
        "        \n",
        "        x_time_data = np.zeros((batch_size, time_steps//subsample, 32))\n",
        "        yy = []\n",
        "        for i in range(batch_size):\n",
        "            random_index = np.random.randint(0, len(x_data)-time_steps)\n",
        "            x_time_data[i] = x_data[random_index:random_index+time_steps:subsample]\n",
        "            yy.append(y_data[random_index + time_steps])\n",
        "        yy = np.asarray(yy)\n",
        "        yield x_time_data.reshape((x_time_data.shape[0],x_time_data.shape[1], x_time_data.shape[2], 1)), yy\n",
        "        \n",
        "        \n",
        "    \n",
        "model.fit_generator(generator(32), steps_per_epoch = 5000, epochs = 3)"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "cu_dnnlstm_7 (CuDNNLSTM)     (None, 10, 128)           82944     \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_8 (CuDNNLSTM)     (None, 128)               132096    \n",
            "_________________________________________________________________\n",
            "batch_normalization_66 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 32)                4128      \n",
            "_________________________________________________________________\n",
            "batch_normalization_67 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 6)                 198       \n",
            "=================================================================\n",
            "Total params: 220,006\n",
            "Trainable params: 219,686\n",
            "Non-trainable params: 320\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            " 234/5000 [>.............................] - ETA: 2:36 - loss: 0.2541 - acc: 0.1716"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1211/5000 [======>.......................] - ETA: 1:27 - loss: 0.2431 - acc: 0.2265"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-180-d97ad2dd995c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1313\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2228\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2229\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "eDajD6dD3Skv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Test phase.\n",
        "Here we load some unseen data. We use, as a metric, the roc_auc_score, as defined in the Kaggle challenge. We create a new generator, and sample this metric on num_test new data points, averaging the score."
      ]
    },
    {
      "metadata": {
        "id": "sha49lFS5NmQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc, roc_auc_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3jtFc22tyklL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "x_val_path = \"train/subj4_series7_data.csv\"\n",
        "y_val_path = \"train/subj4_series7_events.csv\"\n",
        "x_val_data = []\n",
        "with open(x_val_path) as file:\n",
        "    for line in file:\n",
        "        if line[0] == \"i\": continue\n",
        "        line_array = []\n",
        "        for word in range(len(line.split(','))-1):\n",
        "            word+=1\n",
        "            line_array.append(int(line.split(',')[word]))\n",
        "        line_array = np.asarray(line_array)\n",
        "        x_val_data.append(line_array)\n",
        "x_val_data = np.asarray(x_val_data)    \n",
        "        \n",
        "  \n",
        "y_val_data = []\n",
        "with open(y_val_path) as file:\n",
        "    for line in file:\n",
        "        if line[0] == \"i\": continue\n",
        "        line_array = []\n",
        "        for word in range(len(line.split(','))-1):\n",
        "            word+=1\n",
        "            line_array.append(int(line.split(',')[word]))\n",
        "        line_array = np.asarray(line_array)\n",
        "        y_val_data.append(line_array)\n",
        "y_val_data = np.asarray(y_val_data)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fEVzINF4y8XJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def val_generator():\n",
        "    while 1:\n",
        "        batch_size = 1\n",
        "        x_time_data = np.zeros((batch_size, time_steps//subsample, 32))\n",
        "        yy = []\n",
        "        for i in range(batch_size):\n",
        "            random_index = np.random.randint(0, len(x_val_data)-time_steps)\n",
        "            x_time_data[i] = x_val_data[random_index:random_index+time_steps:subsample]\n",
        "            yy.append(y_val_data[random_index + time_steps])\n",
        "        yy = np.asarray(yy)\n",
        "        yield x_time_data.reshape((x_time_data.shape[0],x_time_data.shape[1], x_time_data.shape[2], 1)), yy\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "laZHiRUeuYQ6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf69b125-4231-4b46-c34b-c5d247efd271",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525539252101,
          "user_tz": -120,
          "elapsed": 5406,
          "user": {
            "displayName": "Norman Di Palo",
            "photoUrl": "//lh5.googleusercontent.com/-hwpkiYzSctg/AAAAAAAAAAI/AAAAAAAAAkg/I59DXttMdKw/s50-c-k-no/photo.jpg",
            "userId": "109405231849327541723"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "gen_data = val_generator()\n",
        "scores = []\n",
        "num_test = 1000\n",
        "for i in range(num_test):\n",
        "  x_test, y_test = next(gen_data)\n",
        "  while not 1 in y_test:\n",
        "    #print(y_test)\n",
        "    x_test, y_test = next(gen_data)\n",
        "\n",
        " # print(y_test)\n",
        "  y_out = model.predict(x_test).reshape((6,1))\n",
        " # print(y_out)\n",
        "  scores.append(roc_auc_score(y_test.reshape((6,1)), y_out))\n",
        "scores = np.asarray(scores)\n",
        "print(\"Mean \", np.mean(scores))"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean  0.8485916666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0-eKXupZvhID",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "34d51edd-11e7-4f55-ad03-0c132d40bf2c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525531786038,
          "user_tz": -120,
          "elapsed": 1433,
          "user": {
            "displayName": "Norman Di Palo",
            "photoUrl": "//lh5.googleusercontent.com/-hwpkiYzSctg/AAAAAAAAAAI/AAAAAAAAAkg/I59DXttMdKw/s50-c-k-no/photo.jpg",
            "userId": "109405231849327541723"
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    }
  ]
}